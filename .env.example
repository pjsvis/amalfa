# AMALFA Environment Configuration
# Copy this file to .env and fill in your actual values

# ============================================
# LLM Provider API Keys
# ============================================

# Google Gemini API Key
# Get from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your-gemini-api-key-here

# OpenRouter API Key
# Get from: https://openrouter.ai/keys
OPENROUTER_API_KEY=your-openrouter-api-key-here

# Mistral AI API Key
# Get from: https://console.mistral.ai/
MISTRAL_API_KEY=your-mistral-api-key-here

# ============================================
# LangExtract Configuration
# ============================================

# LangExtract Provider (ollama, gemini, openrouter)
# Default: ollama
LANGEXTRACT_PROVIDER=ollama

# ============================================
# Database Configuration
# ============================================

# Database Path (relative to project root)
# Default: .amalfa/resonance.db
DATABASE_PATH=.amalfa/resonance.db

# ============================================
# Development Settings
# ============================================

# Enable Debug Logging
# Default: false
DEBUG=false

# Log Level (error, warn, info, debug)
# Default: info
LOG_LEVEL=info

# ============================================
# Security Notes
# ============================================

# 1. NEVER commit .env file to version control
# 2. Use strong, unique API keys for each service
# 3. Rotate API keys regularly
# 4. Use different keys for dev/staging/production
# 5. Monitor API usage and costs

# ============================================
# Ollama Configuration
# ============================================

# Ollama uses local and remote models via localhost:11434
# No API key required - uses your Ollama account automatically
# Local models: Run entirely on your machine (private, slow)
# Remote models: Proxied to ollama.com (fast, requires internet)

# Example local model:
# ollama pull mistral-nemo:latest

# Example remote model:
# ollama pull nemotron-3-nano:30b-cloud

# Configure in amalfa.config.json:
# {
#   "langExtract": {
#     "provider": "ollama",
#     "ollama": {
#       "host": "http://localhost:11434",
#       "model": "nemotron-3-nano:30b-cloud"  // or "mistral-nemo:latest"
#     }
#   }
# }
